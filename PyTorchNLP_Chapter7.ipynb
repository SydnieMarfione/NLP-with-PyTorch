{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from argparse import Namespace\n",
        "from collections import Counter\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm_notebook"
      ],
      "metadata": {
        "id": "olaWpnppbaUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQThp4OGsQtC"
      },
      "outputs": [],
      "source": [
        "class SurnameDataset(Dataset):\n",
        "  @classmethod\n",
        "  def load_dataset_and_make_vectorize(cls, surname_csv):\n",
        "\n",
        "    surname_df = pd.read_csv(surname_csv)\n",
        "    return cls (surname_df, SurnameVectorizer.from_dataframe(surname_df))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    row = self._target_df.iloc[index]\n",
        "\n",
        "    from_vector, to_vector = \\\n",
        "      self._vectorizer.vectorize(row.surname, self._max_seq_length)\n",
        "\n",
        "    nationality_index = \\\n",
        "      self._vectorizer.vectorize(row.surname, self._max_seq_length)\n",
        "\n",
        "    return {'x_data': from_vector,\n",
        "            'y_target': to_vector,\n",
        "            'class_index': nationality_index}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SurnameVectorizer(object):\n",
        "  def vectorize(self, surname, vector_length=-1):\n",
        "\n",
        "    indices = [self.char_vocab.begin_seq_index]\n",
        "    indices.extend(self.char_vocab.lookup_token(token) for token in surname)\n",
        "    indices.append(self.char_vocab.end_seq_index)\n",
        "\n",
        "    if vector_length < 0:\n",
        "      vector_length = len(indices) -1\n",
        "\n",
        "    from_vector = np.zeros(vector_length, dtype=np.int64)\n",
        "    from_indices = indices[:-1]\n",
        "    from_vector[:len(from_indices)] = from_indices\n",
        "    from_vector[len(to_indices):] = self.char_vocab.mask_index\n",
        "\n",
        "    to_vector = np.empty(vector_length, dtype=np.int64)\n",
        "    to_indices = indices[1:]\n",
        "    to_vector[:len(to_indices)] = to_indices\n",
        "    to_vector [len(to_indices):] = self.char_vocab.mask_index\n",
        "\n",
        "    return from_vector, to_vector\n",
        "\n",
        "  @classmethod\n",
        "  def from_dataframe(cls, surname_df):\n",
        "\n",
        "      char_vocab = SequenceVocabulary()\n",
        "      nationality_vocab = Vocabulary()\n",
        "      for index, row in surname_df.iterows():\n",
        "        for char in row.surname:\n",
        "            char_vocab.add_token(char)\n",
        "        nationality_vocab.add_token(row.nationality)\n",
        "\n",
        "        return cls(char_vocab, nationality_vocab)"
      ],
      "metadata": {
        "id": "EhwztSlcbhyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SurnameGenerationMoodel(nn.Module):\n",
        "  def __init__(self, char_embedding_size, char_vocab_size, rnn_hidden_size,\n",
        "               batch_first=True, padding_idx=0, dropout_p=0.5):\n",
        "\n",
        "    super(SurnameGenerationModel, self).__init__()\n",
        "\n",
        "    self.char_emb = nn.Embedding(num_embeddings=char_vocab_size,\n",
        "                                 embedding_dim=char_embedding_size,\n",
        "                                 padding_idx=padding_idx)\n",
        "\n",
        "    self.rnn = nn.Embedding(num_embeddings=char_vocab_size,\n",
        "                            embedding_dim=char_embedding_size,\n",
        "                            padding_idx=padding_idx)\n",
        "    self.fc = nn.Linear(in_features=rnn_hidden_size,\n",
        "                        out_features=char_vocab_size)\n",
        "\n",
        "    self._dropout_p = dropout_p\n",
        "\n",
        "  def forward(self, x_in, apply_softmax=False):\n",
        "\n",
        "    x_embedded = self.char_emb(x_in)\n",
        "\n",
        "    y_out, _ = self.rnn(x_embedded)\n",
        "\n",
        "    batch_size, seq_size, feat_size = y_out.shape\n",
        "    y_out = y_out.contiguous().view(batch_size * seq_size, feat_size)\n",
        "\n",
        "    y_out = self.fc(F.dropout(y_out, p=self.dropout_p))\n",
        "\n",
        "    if apply_softmax:\n",
        "      y_out = F.softmax(y_out, dim=1)\n",
        "\n",
        "    new_feat_size = y_out.shape[-1]\n",
        "    y_out = y_out.view(batch_size, seq_size, new_feat_size)\n",
        "\n",
        "    return y_out"
      ],
      "metadata": {
        "id": "IWEEFHPQep_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_sizes(y_pred, y_true):\n",
        "\n",
        "  if len(y_pred.size()) == 3:\n",
        "    y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n",
        "  if len(y_true.size()) == 2:\n",
        "    y_true = y_true.contiguous().view(-1)\n",
        "  return y_pred, y_true\n",
        "\n",
        "def sequence_loss(y_pred, y_true, mask_index):\n",
        "  y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
        "  return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)"
      ],
      "metadata": {
        "id": "Aye1DQdxiT3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = Namespace(\n",
        "    surname_csv='data/surnames/surnames_with_splits.csv',\n",
        "    vectorizer_file=\"vectorizer.json\",\n",
        "    model_state_file=\"model.pth\",\n",
        "    save_dir=\"model_storage/ch7/model1_unconditioned_surname_generation\",\n",
        "    char_embedding_size=32,\n",
        "    rnn_hidden_size=32,\n",
        "    seed=1337,\n",
        "    learning_rate=0.001,\n",
        "    batch_size=128,\n",
        "    num_epochs=100,\n",
        "    early_stopping_criteria=5,\n",
        ")"
      ],
      "metadata": {
        "id": "X6rTqzJimyqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_from_model(model, vectorizer, num_samples=1, sample_size=20,\n",
        "                      temperature=1.0):\n",
        "\n",
        "  begin_seq_index = [vectorizer.char_vocab.begin_seq_index\n",
        "                     for _ in range(num_samples)]\n",
        "\n",
        "  begin_seq_index = torch.tensor(begin_seq_index, dtype=torch.int64).unsqueeze(dim=1)\n",
        "  indices = [begin_seq_index]\n",
        "  h_t = None\n",
        "\n",
        "  for time_step in range(sample_size):\n",
        "    x_t = indices[time_step]\n",
        "    x_emb_t = model.char_emb(x_t)\n",
        "    rnn_out_t, h_t = model.rnn(x_emb_t, h_t)\n",
        "    prediction_vector = model.fc(rnn_out_t.squeeze(dim=1))\n",
        "    probability_vector = F.softmax(prediction_vector / temperature, dim=1)\n",
        "    indices.append(torch.multinomial(probability_vector, num_samples=1))\n",
        "  indices = torch.stack(indices).squeeze().permute(1,0)\n",
        "  return indices"
      ],
      "metadata": {
        "id": "5xR8QIRZp8J9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_samples(sampled_indices, vectorizer):\n",
        "\n",
        "  decoded_surnames = []\n",
        "  vocab = vectorizer.char_vocab\n",
        "\n",
        "  for sample_index in range(sampled_indices.shape[0]):\n",
        "    surname = \"\"\n",
        "    for time_step in range(sampled_indices.shape[1]):\n",
        "      sample_item = sampled_indices[sample_index, time_step].item()\n",
        "      if sample_item == vocab.begin_seq_index:\n",
        "        continue\n",
        "      elif sample_item == vocab.end_seq_index:\n",
        "        break\n",
        "      else:\n",
        "          surname += vocab.lookup_index(sample_item)\n",
        "    decoded_surnames.append(surname)\n",
        "  return decoded_surnames"
      ],
      "metadata": {
        "id": "iriKMNrJr4Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_from_model(model, vectorizer, nationalities, sample_size=20, temperature=1.0):\n",
        "    num_samples = len(nationalities)\n",
        "    begin_seq_index = [vectorizer.char_vocab.begin_seq_index for _ in range(num_samples)]\n",
        "    begin_seq_index = torch.tensor(begin_seq_index, dtype=torch.int64).unsqueeze(dim=1)\n",
        "\n",
        "    indices = [begin_seq_index]\n",
        "    nationality_indices = torch.tensor(nationalities, dtype=torch.int64).unsqueeze(dim=0)\n",
        "    h_t = model.nation_emb(nationality_indices)\n",
        "\n",
        "    for time_step in range(sample_size):\n",
        "        x_t = indices[time_step]\n",
        "        x_emb_t = model.char_emb(x_t)\n",
        "        rnn_out_t, h_t = model.rnn(x_emb_t, h_t)\n",
        "        prediction_vector = model.fc(rnn_out_t.squeeze(dim=1))\n",
        "        probability_vector = F.softmax(prediction_vector / temperature, dim=1)\n",
        "        indices.append(torch.multinomial(probability_vector, num_samples=1))\n",
        "\n",
        "    indices = torch.stack(indices).squeeze().permute(1, 0)\n",
        "    return indices"
      ],
      "metadata": {
        "id": "bE0Sez91tTEZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}