{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm_notebook"
      ],
      "metadata": {
        "id": "0OdsP5QkNP6g"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3zvUKAKRqAdR"
      },
      "outputs": [],
      "source": [
        "class ElmanRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, batch_first=False):\n",
        "    super(ElmanRNN, self).__init__()\n",
        "\n",
        "    self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
        "\n",
        "    self.batch_first = batch_first\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "  def _initialize_hidden(self, batch_size):\n",
        "    return torch.zeros((batch_size, self.hidden_size))\n",
        "\n",
        "  def forward(self, x_in, initial_hidden=None):\n",
        "\n",
        "    if self.batch_first:\n",
        "      batch_size, seq_size, feat_size = x_in.size()\n",
        "      x_in = x_in.permute(1, 0, 2)\n",
        "\n",
        "    else:\n",
        "      seq_size, batch_size, feat_size = x_in.size()\n",
        "\n",
        "    hiddens = []\n",
        "\n",
        "    if initial_hidden is None:\n",
        "      initial_hidden = self._initialize_hidden(batch_size)\n",
        "      initial_hidden = initial_hidden.to(x_in.device)\n",
        "\n",
        "    hidden_t = initial_hidden\n",
        "\n",
        "    for t in range(seq_size):\n",
        "      hidden_t = self.rnn_cell(x_in[t], hidden_t)\n",
        "      hiddens.append(hidden_t)\n",
        "\n",
        "    hiddens = torch.stack(hiddens)\n",
        "\n",
        "    if self.batch_first:\n",
        "      hiddens = hiddens.permute(1, 0, 2)\n",
        "\n",
        "    return hiddens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SurnameDataset(Dataset):\n",
        "  @classmethod\n",
        "  def load_dataset_and_make_vectorize(cls, surname_csv):\n",
        "\n",
        "    surname_df = pd.read_csv(surname_csv)\n",
        "    train_surname_df[surname_df.split=='train']\n",
        "    return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "      row = self._target_df.iloc[index]\n",
        "\n",
        "      surname_vector, vec_length = \\\n",
        "        self._vectorizer.vectorize(row.surname, self._max_seq_length)\n",
        "\n",
        "      nationality_index = \\\n",
        "        self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
        "\n",
        "      return {'x_data': surname_vector,\n",
        "              'y_target': nationality_index,\n",
        "              'x_length': vec_length}"
      ],
      "metadata": {
        "id": "s4GeQtAwOJ0s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SurnameVectorizer(object):\n",
        "  def vectorize(self, surname, vector_length=-1):\n",
        "\n",
        "    indices = [self.char_vocab.begin_seq_index]\n",
        "    indices.extend(self.char_vocab.lookup_token(token)\n",
        "                    for token in surname)\n",
        "    indices.append(self.char_vocab.end_seq_index)\n",
        "\n",
        "    if vector_length < 0:\n",
        "      vector_length = len(indices)\n",
        "\n",
        "    out_vector = np.zeros(vector_lenth, dtype=np.int64)\n",
        "    out_vector[:len(indices)] = indices\n",
        "    out_vector[len(indices):] = self.char_vocab.mask_index\n",
        "\n",
        "    return out_vector, len(indices)\n",
        "\n",
        "  @classmethod\n",
        "  def from_dataframe(cls, surname_df):\n",
        "\n",
        "    char_vocab = SequenceVocabulary()\n",
        "    nationality_vocab = Vocabulary()\n",
        "    for index, row in surname_df.iterrows():\n",
        "      for char in row.surname:\n",
        "        char_vocab.add_token(char)\n",
        "      nationality_vocab.add_token(row.nationality)\n",
        "\n",
        "    return cls(char_vocab, nationality_vocab)"
      ],
      "metadata": {
        "id": "FWaKXuFseXBW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SurnameClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_size, num_embeddings, num_classes,\n",
        "                 rnn_hidden_size, batch_first=True, padding_idx=0):\n",
        "\n",
        "        super(SurnameClassifier, self).__init__()\n",
        "\n",
        "        self.emb = nn.Embedding(num_embeddings=num_embeddings,\n",
        "                                embedding_dim=embedding_size,\n",
        "                                padding_idx=padding_idx)\n",
        "        self.rnn = ElmanRNN(input_size=embedding_size,\n",
        "                             hidden_size=rnn_hidden_size,\n",
        "                             batch_first=batch_first)\n",
        "        self.fc1 = nn.Linear(in_features=rnn_hidden_size,\n",
        "                         out_features=rnn_hidden_size)\n",
        "        self.fc2 = nn.Linear(in_features=rnn_hidden_size,\n",
        "                          out_features=num_classes)\n",
        "\n",
        "    def forward(self, x_in, x_lengths=None, apply_softmax=False):\n",
        "\n",
        "        x_embedded = self.emb(x_in)\n",
        "        y_out = self.rnn(x_embedded)\n",
        "\n",
        "        if x_lengths is not None:\n",
        "            y_out = column_gather(y_out, x_lengths)\n",
        "        else:\n",
        "            y_out = y_out[:, -1, :]\n",
        "\n",
        "        y_out = F.relu(self.fc1(F.dropout(y_out, 0.5)))\n",
        "        y_out = self.fc2(F.dropout(y_out, 0.5))\n",
        "\n",
        "        if apply_softmax:\n",
        "            y_out = F.softmax(y_out, dim=1)\n",
        "\n",
        "        return y_out"
      ],
      "metadata": {
        "id": "RJTpWA2-gwGu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def column_gather(y_out, x_lengths):\n",
        "\n",
        "    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
        "\n",
        "    out = []\n",
        "    for batch_index, column_index in enumerate(x_lengths):\n",
        "        out.append(y_out[batch_index, column_index])\n",
        "\n",
        "    return torch.stack(out)"
      ],
      "metadata": {
        "id": "lLf6KXZGiJiv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = Namespace(\n",
        "    # data and path info\n",
        "    surname_csv=\"data/surnames/surnames_with_splits.csv\",\n",
        "    vectorizer_file=\"vectorizer.json\",\n",
        "    model_state_file=\"model.pth\",\n",
        "    save_dir=\"model_storage/ch6/surname_classification\",\n",
        "    # model hyper parameter\n",
        "    char_embedding_size=100,\n",
        "    rnn_hidden_size=64,\n",
        "    # training hyper parameter\n",
        "    num_epochs=100,\n",
        "    learning_rate=1e-3,\n",
        "    batch_size=64,\n",
        "    seed=1337,\n",
        "    early_stopping_criteria=5,\n",
        "    # runtime hyper parameter\n",
        "    cuda=True,\n",
        "    catch_keyboard_interrupt=True,\n",
        "    reload_from_files=False,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        ")"
      ],
      "metadata": {
        "id": "2Dj7tEaViP9w"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}