{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "snbG5ntMlCwo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from argparse import Namespace\n",
        "from collections import Counter\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm_notebook"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NMTVectorizer(object):\n",
        "\n",
        "  def __init__(self, source_vocab, target_vocab, max_source_length,\n",
        "               max_target_length):\n",
        "\n",
        "    self.source_vocab = source_vocab\n",
        "    self.target_vocab = target_vocab\n",
        "\n",
        "    self.max_source_length = max_source_length\n",
        "    self.target_vocab = target_vocab\n",
        "\n",
        "    self.max_source_lenth = max_source_length\n",
        "    self.max_target_length = max_target_length\n",
        "\n",
        "@classmethod\n",
        "def from_dataframe(cls, bitext_def):\n",
        "\n",
        "  source_vocab = SequenceVocabulary()\n",
        "  target_vocab = SequenceVocabulary()\n",
        "  max_source_length, max_target_length = 0,0\n",
        "\n",
        "  for _, row in bitext_df.itterows():\n",
        "      source_tokens = row[\"source\"]\n",
        "      if len(source_tokens) > max_source_length:\n",
        "          max_source_length = len(source_tokens)\n",
        "      for token in source_tokens:\n",
        "          source_vocab.add_token(token)\n",
        "\n",
        "      target_tokens = row[\"target_language\"].split(\" \")\n",
        "      if len(target_tokens) > max_target_length:\n",
        "        max_target_length = len(target_tokens)\n",
        "      for token in target_tokens:\n",
        "        target_vocab.add_token(token)\n",
        "\n",
        "  return cls(source_vocab, target_vocab, max_source_length,\n",
        "            max_target_length)"
      ],
      "metadata": {
        "id": "QgYraDWzlfWt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NMTVectorizer(object):\n",
        "  def _vectorize(self,indices, vector_length=-1, mask_index=0):\n",
        "\n",
        "    if vector_length < 0:\n",
        "      vector_length = len(indices)\n",
        "    vector = np.zeros(vector_length, dtype=np.int64)\n",
        "    vector[:len(indices)] = indices\n",
        "    vector[len(indices):] = mask_index\n",
        "    return vector\n",
        "\n",
        "  def _get_source_indices(self, text):\n",
        "\n",
        "    indices = [self.source_vocab.begin_seq_index]\n",
        "    indices.extend(self.source_vocab.lookup_token(token)\n",
        "                  for token in text.split(\" \"))\n",
        "    indices.append(self.source_vocab.end_seq__index)\n",
        "    return indices\n",
        "\n",
        "  def _get_target_indices(self,text):\n",
        "\n",
        "    indices = [self.target_vocab.lookup_token(token)\n",
        "              for token in text.split(\" \")]\n",
        "    x_indices = [self.target_vocab.begin_seq_index] + indices\n",
        "    y_indices = indices + [self.target_vocab.end_seq_index]\n",
        "    return x_indices, y_indices\n",
        "\n",
        "  def vectorize(self, source_text, target_text, use_dataset_max_lengths=True):\n",
        "\n",
        "    source_vector_length = -1\n",
        "    target_vector_length = -1\n",
        "\n",
        "    if use_dataset_max_lengths:\n",
        "      source_vector_length = self.max_source_length + 2\n",
        "      target_vector_length = self.max_target_length + 1\n",
        "\n",
        "    source_indices = self._get_source_indices(source_text)\n",
        "    source_vector_length = self._vectorize(source_indices,\n",
        "                                           vector_length=source_vector_length,\n",
        "                                           mask_index=self.source_vocab.mask_index)\n",
        "    target_x_indices, target_y_indices = self._get_target_indices\n",
        "    (target_text)\n",
        "    target_x__vector = self._vectorize(target_x_indices,\n",
        "                                       vector_length=target_vector_length,\n",
        "                                       mask_index=self.target_vocab.mask_index)\n",
        "\n",
        "    target_y_vector = self._vectorize(target_y_indices,\n",
        "                                      vector_length=target_vector_length,\n",
        "                                      mask_index=self.target_vocab.mask_index)\n",
        "    return {\"source_vector\": source_vector,\n",
        "            \"target_x_vector\": target_x_vector,\n",
        "            \"target_y_vector\": target_y_vector,\n",
        "            \"source_length\": len(source_indices)}"
      ],
      "metadata": {
        "id": "oNud4qhTtuhu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_nmt_batches(dataset, batch_size, shuffle=True,\n",
        "                            drop_last=True, device=\"cpu\"):\n",
        "\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
        "                            shuffle=shuffle, drop_last=drop_last)\n",
        "\n",
        "    for data_dict in dataloader:\n",
        "        lengths = data_dict['x_source_length'].numpy()\n",
        "        sorted_length_indices = lengths.argsort()[::-1].tolist()\n",
        "\n",
        "        out_data_dict = {}\n",
        "        for name, tensor in data_dict.items():\n",
        "            out_data_dict[name] = data_dict[name][sorted_length_indices].to(device)\n",
        "        yield out_data_dict"
      ],
      "metadata": {
        "id": "Fhd8fnkB-GFp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NMTModel(nn.Module):\n",
        "\n",
        "    def __init__(self, source_vocab_size, source_embedding_size,\n",
        "                 target_vocab_size, target_embedding_size, encoding_size,\n",
        "                 target_bos_index):\n",
        "\n",
        "        super(NMTModel, self).__init__()\n",
        "        self.encoder = NMTEncoder(num_embeddings=source_vocab_size,\n",
        "                                  embedding_size=source_embedding_size,\n",
        "                                  rnn_hidden_size=encoding_size)\n",
        "        decoding_size = encoding_size * 2\n",
        "        self.decoder = NMTDecoder(num_embeddings=target_vocab_size,\n",
        "                                  embedding_size=target_embedding_size,\n",
        "                                  rnn_hidden_size=decoding_size,\n",
        "                                  bos_index=target_bos_index)\n",
        "\n",
        "    def forward(self, x_source, x_source_lengths, target_sequence):\n",
        "\n",
        "        encoder_state, final_hidden_states = self.encoder(x_source, x_source_lengths)\n",
        "        decoded_states = self.decoder(encoder_state=encoder_state,\n",
        "                                      initial_hidden_state=final_hidden_states,\n",
        "                                      target_sequence=target_sequence)\n",
        "        return decoded_states"
      ],
      "metadata": {
        "id": "goV0r_hUAm_f"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NMTEncoder(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size):\n",
        "\n",
        "        super(NMTEncoder, self).__init__()\n",
        "\n",
        "        self.source_embedding = nn.Embedding(num_embeddings, embedding_size, padding_idx=0)\n",
        "        self.birnn = nn.GRU(embedding_size, rnn_hidden_size, bidirectional=True, batch_first=True)\n",
        "\n",
        "    def forward(self, x_source, x_lengths):\n",
        "\n",
        "        x_embedded = self.source_embedding(x_source)\n",
        "\n",
        "        x_packed = pack_padded_sequence(x_embedded, x_lengths.detach().cpu().numpy(),\n",
        "                                        batch_first=True)\n",
        "\n",
        "        x_birnn_out, x_birnn_h  = self.birnn(x_packed)\n",
        "\n",
        "        x_birnn_h = x_birnn_h.permute(1, 0, 2)\n",
        "\n",
        "        x_birnn_h = x_birnn_h.contiguous().view(x_birnn_h.size(0), -1)\n",
        "\n",
        "        x_unpacked, _ = pad_packed_sequence(x_birnn_out, batch_first=True)\n",
        "\n",
        "        return x_unpacked, x_birnn_h"
      ],
      "metadata": {
        "id": "JIjz-9C0A-eF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abcd_padded = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
        "efg_padded = torch.tensor([5, 6, 7, 0], dtype=torch.float32)\n",
        "h_padded = torch.tensor([8, 0, 0, 0], dtype=torch.float32)\n",
        "\n",
        "padded_tensor = torch.stack([abcd_padded, efg_padded, h_padded])\n",
        "\n",
        "print(padded_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6jDL4jHBLkX",
        "outputId": "2bcb817a-bbc2-41e1-aad3-ea942bc77b1d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2., 3., 4.],\n",
            "        [5., 6., 7., 0.],\n",
            "        [8., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lengths = [4, 3, 1]\n",
        "packed_tensor = pack_padded_sequence(padded_tensor, lengths,\n",
        "                                          batch_first=True)\n",
        "packed_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x9hgxQFB_Gu",
        "outputId": "5ab739a6-ab16-4f61-f910-6877ecffa167"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([1., 5., 8., 2., 6., 3., 7., 4.]), batch_sizes=tensor([3, 2, 2, 1]), sorted_indices=None, unsorted_indices=None)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unpacked_tensor, unpacked_lengths = \\\n",
        "  pad_packed_sequence(packed_tensor, batch_first=True)\n",
        "\n",
        "print(unpacked_tensor)\n",
        "print(unpacked_lengths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNYotrlbCgQ3",
        "outputId": "777d2e3f-938c-48fb-f76e-547555146366"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2., 3., 4.],\n",
            "        [5., 6., 7., 0.],\n",
            "        [8., 0., 0., 0.]])\n",
            "tensor([4, 3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NMTDecoder(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size, bos_index):\n",
        "\n",
        "        super(NMTDecoder, self).__init__()\n",
        "        self._rnn_hidden_size = rnn_hidden_size\n",
        "        self.target_embedding = nn.Embedding(num_embeddings=num_embeddings,\n",
        "                                             embedding_dim=embedding_size,\n",
        "                                             padding_idx=0)\n",
        "        self.gru_cell = nn.GRUCell(embedding_size + rnn_hidden_size,\n",
        "                                   rnn_hidden_size)\n",
        "        self.hidden_map = nn.Linear(rnn_hidden_size, rnn_hidden_size)\n",
        "        self.classifier = nn.Linear(rnn_hidden_size * 2, num_embeddings)\n",
        "        self.bos_index = bos_index\n",
        "\n",
        "    def _init_indices(self, batch_size):\n",
        "\n",
        "        return torch.ones(batch_size, dtype=torch.int64) * self.bos_index\n",
        "\n",
        "    def _init_context_vectors(self, batch_size):\n",
        "\n",
        "        return torch.zeros(batch_size, self._rnn_hidden_size)\n",
        "\n",
        "    def forward(self, encoder_state, initial_hidden_state, target_sequence):\n",
        "\n",
        "        target_sequence = target_sequence.permute(1, 0)\n",
        "        output_sequence_size = target_sequence.size(0)\n",
        "\n",
        "        h_t = self.hidden_map(initial_hidden_state)\n",
        "\n",
        "        batch_size = encoder_state.size(0)\n",
        "\n",
        "        context_vectors = self._init_context_vectors(batch_size)\n",
        "\n",
        "        y_t_index = self._init_indices(batch_size)\n",
        "\n",
        "        h_t = h_t.to(encoder_state.device)\n",
        "        y_t_index = y_t_index.to(encoder_state.device)\n",
        "        context_vectors = context_vectors.to(encoder_state.device)\n",
        "\n",
        "        output_vectors = []\n",
        "        self._cached_p_attn = []\n",
        "        self._cached_ht = []\n",
        "        self._cached_decoder_state = encoder_state.cpu().detach().numpy()\n",
        "\n",
        "        for i in range(output_sequence_size):\n",
        "            y_t_index = target_sequence[i]\n",
        "\n",
        "            y_input_vector = self.target_embedding(y_t_index)\n",
        "            rnn_input = torch.cat([y_input_vector, context_vectors], dim=1)\n",
        "\n",
        "            h_t = self.gru_cell(rnn_input, h_t)\n",
        "            self._cached_ht.append(h_t.cpu().detach().numpy())\n",
        "\n",
        "            context_vectors, p_attn, _ = verbose_attention(encoder_state_vectors=encoder_state,\n",
        "                                                           query_vector=h_t)\n",
        "\n",
        "            self._cached_p_attn.append(p_attn.cpu().detach().numpy())\n",
        "\n",
        "            prediction_vector = torch.cat((context_vectors, h_t), dim=1)\n",
        "            score_for_y_t_index = self.classifier(F.dropout(prediction_vector, 0.3))\n",
        "\n",
        "            output_vectors.append(score_for_y_t_index)"
      ],
      "metadata": {
        "id": "A1V18D29Czdi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verbose_attention(encoder_state_vectors, query_vector):\n",
        "\n",
        "    batch_size, num_vectors, vector_size = encoder_state_vectors.size()\n",
        "    vector_scores = torch.sum(encoder_state_vectors * query_vector.view(batch_size, 1, vector_size),\n",
        "                              dim=2)\n",
        "    vector_probabilities = F.softmax(vector_scores, dim=1)\n",
        "    weighted_vectors = encoder_state_vectors * vector_probabilities.view(batch_size, num_vectors, 1)\n",
        "    context_vectors = torch.sum(weighted_vectors, dim=1)\n",
        "    return context_vectors, vector_probabilities, vector_scores\n",
        "\n",
        "def terse_attention(encoder_state_vectors, query_vector):\n",
        "\n",
        "    vector_scores = torch.matmul(encoder_state_vectors, query_vector.unsqueeze(dim=2)).squeeze()\n",
        "    vector_probabilities = F.softmax(vector_scores, dim=-1)\n",
        "    context_vectors = torch.matmul(encoder_state_vectors.transpose(-2, -1),\n",
        "                                   vector_probabilities.unsqueeze(dim=2)).squeeze()\n",
        "    return context_vectors, vector_probabilities"
      ],
      "metadata": {
        "id": "DTJF33t2DJL9"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}